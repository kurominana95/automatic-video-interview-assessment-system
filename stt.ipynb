{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JolhLB8P0wNO",
        "outputId": "b0415949-2f68-488f-b6d1-e3e0493d3a0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
            "Collecting ffmpeg-python\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting jiwer\n",
            "  Downloading jiwer-4.0.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown) (4.13.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from gdown) (3.20.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.12/dist-packages (from gdown) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.12/dist-packages (from ffmpeg-python) (1.0.0)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from jiwer) (8.3.1)\n",
            "Collecting rapidfuzz>=3.9.7 (from jiwer)\n",
            "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /packages/30/83/80d22997acd928eda7deadc19ccd15883904622396d6571e935993e0453a/rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata\u001b[0m\u001b[33m\n",
            "\u001b[0m  Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (2.8)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2025.11.12)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Downloading jiwer-4.0.0-py3-none-any.whl (23 kB)\n",
            "Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, ffmpeg-python, jiwer\n",
            "Successfully installed ffmpeg-python-0.2.0 jiwer-4.0.0 rapidfuzz-3.14.3\n",
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-xxzf403l\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-xxzf403l\n",
            "  Resolved https://github.com/openai/whisper.git to commit c0d2f624c09dc18e709e37c2ad90c039a4eb72a2\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (10.8.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (2.0.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (0.12.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (2.9.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (3.5.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->openai-whisper==20250625) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper==20250625) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper==20250625) (2.32.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (1.11.1.6)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->openai-whisper==20250625) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->openai-whisper==20250625) (3.0.3)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20250625-py3-none-any.whl size=803979 sha256=dfe233d315de2a269c1f633150f2a49274c47e8c7e1ad83caa937c833a974f36\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-fdafsqbx/wheels/c3/03/25/5e0ba78bc27a3a089f137c9f1d92fdfce16d06996c071a016c\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: openai-whisper\n",
            "Successfully installed openai-whisper-20250625\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\n",
            "Torch device: cuda\n"
          ]
        }
      ],
      "source": [
        "# CELL 1 — Install Dependencies & Config\n",
        "\n",
        "!pip install gdown ffmpeg-python jiwer transformers\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!apt-get install ffmpeg -y\n",
        "\n",
        "import os, json, math, datetime\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import ffmpeg\n",
        "import gdown\n",
        "import whisper\n",
        "import torch\n",
        "from jiwer import wer\n",
        "\n",
        "print(\"Torch device:\", torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "\n",
        "# CONFIG — list video Drive IDs\n",
        "DRIVE_IDS = [\n",
        "    \"1pVqgMe9g3_3AVxpf2odq3kWfJSOGyFkc\",\n",
        "    \"1bt2Sx5FVdr4ONV_Bz-0vF6ym2bar0xF5\",\n",
        "    \"1ye5yChiUE4lY0KL5Xotc-dJWI5W-WwYe\",\n",
        "    \"1DtY3Pnxv5rFSbENYHxOyW-8TlqZLECq0\",\n",
        "    \"1nykD4nX2A94DrSwf9PdZi-4abXmxZcKP\"\n",
        "]\n",
        "\n",
        "QUESTIONS = {\n",
        "    1: \"Can you share any specific challenges you faced while working on certification and how you overcame them?\",\n",
        "    2: \"Can you describe your experience with transfer learning in TensorFlow? How did it benefit your projects?\",\n",
        "    3: \"Describe a complex TensorFlow model you have built and the steps you took to ensure its accuracy and efficiency.\",\n",
        "    4: \"Explain how to implement dropout in a TensorFlow model and the effect it has on training.\",\n",
        "    5: \"Describe the process of building a convolutional neural network (CNN) using TensorFlow for image classification.\"\n",
        "}\n",
        "\n",
        "OUT_DIR = \"capstone_output\"\n",
        "VID_DIR = os.path.join(OUT_DIR, \"videos\")\n",
        "AUD_DIR = os.path.join(OUT_DIR, \"audios\")\n",
        "os.makedirs(VID_DIR, exist_ok=True)\n",
        "os.makedirs(AUD_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 2 — Download raw video files\n",
        "\n",
        "def download_drive(id_, dest):\n",
        "    url = f\"https://drive.google.com/uc?id={id_}\"\n",
        "    print(\"Downloading:\", id_)\n",
        "    gdown.download(url, dest, quiet=False)\n",
        "\n",
        "video_paths = []\n",
        "for i, fid in enumerate(DRIVE_IDS, start=1):\n",
        "    out = os.path.join(VID_DIR, f\"interview_question_{i}.mp4\")\n",
        "    download_drive(fid, out)\n",
        "    video_paths.append(out)\n",
        "\n",
        "print(\"Downloaded files:\", video_paths)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIJyb51q0yU0",
        "outputId": "afe35ddf-4f32-4b09-9db4-c967c80238c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 1pVqgMe9g3_3AVxpf2odq3kWfJSOGyFkc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1pVqgMe9g3_3AVxpf2odq3kWfJSOGyFkc\n",
            "To: /content/capstone_output/videos/interview_question_1.mp4\n",
            "100%|██████████| 18.0M/18.0M [00:00<00:00, 38.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 1bt2Sx5FVdr4ONV_Bz-0vF6ym2bar0xF5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1bt2Sx5FVdr4ONV_Bz-0vF6ym2bar0xF5\n",
            "To: /content/capstone_output/videos/interview_question_2.mp4\n",
            "100%|██████████| 22.0M/22.0M [00:00<00:00, 63.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 1ye5yChiUE4lY0KL5Xotc-dJWI5W-WwYe\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ye5yChiUE4lY0KL5Xotc-dJWI5W-WwYe\n",
            "To: /content/capstone_output/videos/interview_question_3.mp4\n",
            "100%|██████████| 22.9M/22.9M [00:00<00:00, 54.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 1DtY3Pnxv5rFSbENYHxOyW-8TlqZLECq0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1DtY3Pnxv5rFSbENYHxOyW-8TlqZLECq0\n",
            "To: /content/capstone_output/videos/interview_question_4.mp4\n",
            "100%|██████████| 23.1M/23.1M [00:00<00:00, 37.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 1nykD4nX2A94DrSwf9PdZi-4abXmxZcKP\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1nykD4nX2A94DrSwf9PdZi-4abXmxZcKP\n",
            "To: /content/capstone_output/videos/interview_question_5.mp4\n",
            "100%|██████████| 23.0M/23.0M [00:00<00:00, 51.9MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded files: ['capstone_output/videos/interview_question_1.mp4', 'capstone_output/videos/interview_question_2.mp4', 'capstone_output/videos/interview_question_3.mp4', 'capstone_output/videos/interview_question_4.mp4', 'capstone_output/videos/interview_question_5.mp4']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 3 — Convert MP4 files to WAV audio\n",
        "\n",
        "def convert_to_wav(mp4_path, wav_path):\n",
        "    (\n",
        "        ffmpeg\n",
        "        .input(mp4_path)\n",
        "        .output(wav_path, ac=1, ar=\"16000\")\n",
        "        .overwrite_output()\n",
        "        .run(quiet=True)\n",
        "    )\n",
        "\n",
        "wav_paths = []\n",
        "for mp4 in video_paths:\n",
        "    base = Path(mp4).stem\n",
        "    wav = os.path.join(AUD_DIR, base + \".wav\")\n",
        "    print(f\"Converting {mp4} -> {wav}\")\n",
        "    convert_to_wav(mp4, wav)\n",
        "    wav_paths.append(wav)\n",
        "\n",
        "print(\"All conversions done.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2oHPuyc06y6",
        "outputId": "2a86e707-c467-45e1-e812-ae3e44484a5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting capstone_output/videos/interview_question_1.mp4 -> capstone_output/audios/interview_question_1.wav\n",
            "Converting capstone_output/videos/interview_question_2.mp4 -> capstone_output/audios/interview_question_2.wav\n",
            "Converting capstone_output/videos/interview_question_3.mp4 -> capstone_output/audios/interview_question_3.wav\n",
            "Converting capstone_output/videos/interview_question_4.mp4 -> capstone_output/audios/interview_question_4.wav\n",
            "Converting capstone_output/videos/interview_question_5.mp4 -> capstone_output/audios/interview_question_5.wav\n",
            "All conversions done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 4 — Load Whisper Model\n",
        "\n",
        "MODEL_NAME = \"large\"   # medium jika RAM kecil\n",
        "print(\"Loading model:\", MODEL_NAME)\n",
        "model = whisper.load_model(MODEL_NAME)\n",
        "print(\"Model loaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B04S-BYX078L",
        "outputId": "6cabdb78-5dc2-4aba-99dd-0665fdbd698f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model: large\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████| 2.88G/2.88G [00:43<00:00, 71.2MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 5 — Compute confidence scores\n",
        "\n",
        "def seg_confidence_from_avglogprob(avg_logprob):\n",
        "    try:\n",
        "        p = math.exp(avg_logprob)\n",
        "    except:\n",
        "        p = 0.0\n",
        "    return float(max(0.0, min(1.0, p)))\n",
        "\n",
        "def overall_confidence_from_segments(segments):\n",
        "    if not segments:\n",
        "        return 0.0\n",
        "    weights = [(s['end'] - s['start']) for s in segments]\n",
        "    confs = [seg_confidence_from_avglogprob(s.get('avg_logprob', -10)) for s in segments]\n",
        "    total_w = sum(weights)\n",
        "    return float(sum(w*c for w,c in zip(weights,confs)) / total_w)"
      ],
      "metadata": {
        "id": "uH6wSOY-090q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Ground Truth\n",
        "references = {\n",
        "    \"interview_question_1\": \"\"\"share any specific challenges you faced while working on certification and how you overcome them ah, ok actually for the challenges, there are some challenges when I took the certification especially for the project submission that I already working with it the first one is actually to meet the specific accuracy or violation laws for the evaluation matrix and yeah actually that's just need to take some trial and error with the different architecture for example like we can try to add more layer, more neurons, changes the neurons or even I also apply the dropout layer so it really helps with the validation loss to become more lower and I think that's one of the biggest challenges that I have while working on this certification.\"\"\",\n",
        "\n",
        "    \"interview_question_2\": \"\"\"Can you describe your experience with transfer learning and time short flow? How do you benefit from the projects? About transfer learning is actually we use existing train model from time short flow for example like VGC16, VGC19 especially for some cases that we need to use deep learning using Keras applications for example like image classification we can use transfer learning models which is that's already train model with exceptionally high accuracy, high performance, even though it's trained with different datasets but it really helps to improve our model performance, model accuracy, model loss and you know for example like mobile net, VGG19, VGG16, efficient net, it will help to improve our our models comparing to the one if you use a traditional CNN model yeah CNN model with the convolutional 2d yeah max pooling and yeah it it's it's quite good actually to use transfer learning it really helps with our model performance to improve our model performance.\"\"\",\n",
        "\n",
        "    \"interview_question_3\": \"\"\"wait what is this? describe a complex TensorFlow model you have built and the steps you took to ensure its accuracy and efficiency hmmm, a complex TensorFlow model you have built and steps you took to ensure its accuracy okay I will take one of my previous project that I use yeah I also use Keras or TensorFlow model it's it is about a celiac disease prediction yeah yeah this is also I use the research project for my undergraduate thesis yeah for my script C and I use this model it's quite challenging even though it's achieved high accuracy with some dense layer, with some drawout layer and trial and error also with the callback function with the neurons but the problem is the dataset is not balanced so it has the imbalanced class datasets and the approach that I use is to just to use the technique called smooth and synthetic oversampling technique with edited nearest neighbor which is basically it's just oversampling and undersampling the datasets it helps with the accuracy.\"\"\",\n",
        "\n",
        "    \"interview_question_4\": \"\"\"explain how you implement dropout in test on flow model and test on training previously i also have implement the dropout layer also in the project submission within this certifications and we can just add the dropout layer for example if I'm not mistaken it's I have used this dropout layer in the the one that the case is image classifications yeah a German traffic something if I'm not wrong I have used this dropout layer in in in the not in the last in the middle of the layer so there's a flattened layer right not flatten the convolutional layer and then the flattened layer and I use that dropout layer which is I use with the rate of 0.2 or 0.5 if I'm not wrong and then the dense layer and the last the output layer right the effect is it will really helps to improve our accuracy and lower our validation loss by turning off some of the previous layer, for example like we have dense layer 64 and the next layer we implement the dropout layer with the rate of 0.5 and it will turn off randomly each epoch of the previous dense layer\"\"\",\n",
        "\n",
        "    \"interview_question_5\": \"\"\"Describe the process of building or configuration of your image application okay, the CNN one, right? so, at the first time, of course, we need to make sure there are split, the image folder is split for each class and then we can use Keras preprocessing image dataset from directory to split the training and the validation dataset of course we can use another set which is the test dataset but yeah okay the next one we can we can just maybe we we need to implement also the image augmentation yeah data data image data augmentation to to make our data set more veritive right for example like we can rotate we can zoom it we can crop it yeah and yeah the last thing yeah of course we we can build our chain model with the convolutional 2D specify the filters the kernel size the LR activation of course the input shape for the first layer and then we can apply the max pooling 2D yeah and the next layer we can just use use conflux 2d, mix pooling and whatever it is and after that we apply the flatten layer and dropout layer if you want and the last thing don't forget to use the dance layer right for the output.\"\"\"\n",
        "}"
      ],
      "metadata": {
        "id": "clG7XrhS0-3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import random\n",
        "\n",
        "# Normalization helpers\n",
        "def normalize_text(text: str) -> str:\n",
        "    return re.sub(r\"[^a-z0-9\\s]\", \" \", text.lower()).strip()\n",
        "\n",
        "def contains_any(text: str, keywords: list) -> bool:\n",
        "    return any(kw in text for kw in keywords)\n",
        "\n",
        "# Helper untuk memilih 1 reason saja\n",
        "def pick_one_reason(reason_text: str) -> str:\n",
        "    lines = [line.strip() for line in reason_text.split(\"\\n\") if line.strip()]\n",
        "    return random.choice(lines) if lines else reason_text\n",
        "\n",
        "\n",
        "RUBRIC = {\n",
        "    1: {\n",
        "        \"levels\": {\n",
        "            4: (\n",
        "                \"Comprehensive and Clear Response.\\n\"\n",
        "                \"Provides a detailed description of specific challenges encountered during the certification.\\n\"\n",
        "                \"Offers clear explanations of how each challenge was overcome.\\n\"\n",
        "                \"Demonstrates strong understanding of technical aspects and problem-solving skills.\\n\"\n",
        "                \"Response is very clear, well-organized, and reflects the learning process.\"\n",
        "            ),\n",
        "            3: (\n",
        "                \"Specific Challenge with Basic Solution.\\n\"\n",
        "                \"Describes at least one specific challenge related to building machine learning models with TensorFlow.\\n\"\n",
        "                \"Provides a basic explanation of how the challenge was overcome.\\n\"\n",
        "                \"Explanation may be brief or lack depth.\\n\"\n",
        "                \"Shows some understanding but lacks detailed insight.\"\n",
        "            ),\n",
        "            2: \"General Challenge Mentioned without Details.\",\n",
        "            1: \"Minimal or Vague Response.\",\n",
        "            0: \"Unanswered\"\n",
        "        },\n",
        "        \"keywords\": {\n",
        "            4: [\"challenge\",\"overcome\",\"solution\",\"error\",\"architecture\",\"certification\"],\n",
        "            3: [\"challenge\",\"problem\",\"difficult\"],\n",
        "            2: [\"problem\"],\n",
        "            1: []\n",
        "        }\n",
        "    },\n",
        "\n",
        "    2: {\n",
        "        \"levels\": {\n",
        "            4: (\n",
        "                \"Comprehensive and Very Clear Response.\\n\"\n",
        "                \"Offers a detailed description of personal experience using transfer learning in TensorFlow.\\n\"\n",
        "                \"Provides specific examples of projects where transfer learning was applied.\\n\"\n",
        "                \"Demonstrates strong understanding of transfer learning concepts and their practical application.\\n\"\n",
        "                \"Explains how transfer learning benefited those projects.\\n\"\n",
        "                \"Response is very clear, well-organized, and reflects deep engagement with the subject.\"\n",
        "            ),\n",
        "            3: (\n",
        "                \"Specific Experience with Basic Explanation.\\n\"\n",
        "                \"Describes personal experience with transfer learning in TensorFlow.\\n\"\n",
        "                \"Provides examples of projects where transfer learning was applied.\\n\"\n",
        "                \"Explains how transfer learning benefited those projects.\\n\"\n",
        "                \"Explanation may be brief or not fully comprehensive insight.\"\n",
        "            ),\n",
        "            2: (\n",
        "                \"General Response with Limited Details.\\n\"\n",
        "                \"Mentions transfer learning or TensorFlow in general terms.\\n\"\n",
        "                \"Provides minimal details about personal experience.\\n\"\n",
        "                \"Does not clearly explain how transfer learning benefited the projects.\\n\"\n",
        "                \"Shows basic understanding but lucks depth and specificity.\"\n",
        "            ),\n",
        "            1: \"Minimal or Vague Response.\",\n",
        "            0: \"Unanswered\"\n",
        "        },\n",
        "        \"keywords\": {\n",
        "            4: [\"transfer learning\",\"mobilenet\",\"vgg\",\"efficient\",\"pretrained\"],\n",
        "            3: [\"transfer learning\"],\n",
        "            2: [\"tensorflow\"],\n",
        "            1: []\n",
        "        }\n",
        "    },\n",
        "\n",
        "    3: {\n",
        "        \"levels\": {\n",
        "            4: (\n",
        "                \"Comprehensive and Very Clear Response.\\n\"\n",
        "                \"Offers a detailed description of a complex TensorFlow model built.\\n\"\n",
        "                \"Provides specific details about the model’s architecture, features, and purpose.\\n\"\n",
        "                \"Clearly explains the steps taken to ensure both accuracy and efficiency, such as preprocessing, model optimization techniques, regularization methods, and performance tuning.\\n\"\n",
        "                \"Demonstrates strong understanding of TensorFlow and machine learning concepts.\\n\"\n",
        "                \"Response is very clear, well-organized, and reflects deep engagement with the subject.\"\n",
        "            ),\n",
        "            3: (\n",
        "                \"Specific Model with Basic Explanation.\\n\"\n",
        "                \"Describes a complex TensorFlow model they have built.\\n\"\n",
        "                \"Provides some details about the model’s architecture or purpose.\\n\"\n",
        "                \"Explains steps taken to ensure accuracy and efficiency, but may lack depth.\\n\"\n",
        "                \"Explaination may be brief or not fully comprehensive.\"\n",
        "            ),\n",
        "            2: (\n",
        "                \"General Response with Limited Details.\\n\"\n",
        "                \"Mentions building a TensorFlow model in general terms.\\n\"\n",
        "                \"Provides minimal details about the model's complexity.\\n\"\n",
        "                \"Does not clearly explain the steps taken to ensure accuracy and efficiency.\\n\"\n",
        "                \"Shows basic understanding but lucks depth and specificity.\"\n",
        "            ),\n",
        "            1: \"Minimal or Vague Response.\",\n",
        "            0: \"Unanswered\"\n",
        "        },\n",
        "        \"keywords\": {\n",
        "            4: [\"accuracy\",\"optimization\",\"regularization\",\"preprocessing\",\"tuning\",\"callback\",\"early stopping\"],\n",
        "            3: [\"model\",\"accuracy\",\"optimization\"],\n",
        "            2: [],\n",
        "            1: []\n",
        "        }\n",
        "    },\n",
        "\n",
        "    4: {\n",
        "        \"levels\": {\n",
        "            4: (\n",
        "                \"Comprehensive and Very Clear Response.\\n\"\n",
        "                \"Provides a detailed explanation of how to implement dropout in a TensorFlow model, including code examples or specific functions (e.g., using tf.keras.layers.Dropout).\\n\"\n",
        "                \"Clearly explains the effect of dropout on training, such as how it helps prevent overfitting by randomly deactivating neurons during training.\\n\"\n",
        "                \"Discusses the impact on model performance, generalization, and possibly mentions considerations like dropout rates.\\n\"\n",
        "                \"Demonstrates strong understanding of TensorFlow and machine learning concepts.\\n\"\n",
        "                \"Response is very clear, well-organized, and reflects deep engagement with the subject.\"\n",
        "            ),\n",
        "            3: (\n",
        "                \"Specific Explanation with Basic Understanding.\\n\"\n",
        "                \"Explains how to implement dropout in a TensorFlow model with some specifics (e.g., mentions using Dropout layer).\\n\"\n",
        "                \"Describes the general effect of dropout on training, such as preventing overfitting.\\n\"\n",
        "                \"May omit some details about implementation or effects.\\n\"\n",
        "                \"Demonstrates a reasonable understanding but lacks comprehensive insight.\"\n",
        "            ),\n",
        "            2: \"General Response with Limited Details.\",\n",
        "            1: \"Minimal or Vague Response.\",\n",
        "            0: \"Unanswered\"\n",
        "        },\n",
        "        \"keywords\": {\n",
        "            4: [\"dropout\", \"rate\", \"overfitting\", \"random\", \"neuron\", \"impact\",\"deactivating\", \"turning\", \"off\"],\n",
        "            3: [\"dropout\",\"prevent overfitting\"],\n",
        "            2: [\"dropout\"],\n",
        "            1: []\n",
        "        }\n",
        "    },\n",
        "\n",
        "    5: {\n",
        "        \"levels\": {\n",
        "            4: (\n",
        "                \"Comprehensive and Very Clear Response.\\n\"\n",
        "                \"Provides a detailed, step-by-step description of building a CNN in TensorFlow for image classification.\\n\"\n",
        "                \"Covers all key components, including data loading and preprocessing, defining the CNN layers (convolutional, pooling, activation functions), compiling the model with appropriate loss function and optimizer, training the model, and evaluating performance.\\n\"\n",
        "                \"May include code examples or specific TensorFlow functions used.\\n\"\n",
        "                \"Demonstrates strong understanding of CNNs and TensorFlow.\\n\"\n",
        "                \"Response is very clear, well-organized, and reflects deep engagement with the subject.\"\n",
        "            ),\n",
        "            3: (\n",
        "                \"Specific Explanation with Basic Understanding.\\n\"\n",
        "                \"Describes the process of building a CNN in TensorFlow with some specifics.\\n\"\n",
        "                \"Includes key steps such as data preprocessing, defining the CNN architecture, compiling the model, and training.\\n\"\n",
        "                \"May lack comprehensive detail or omit some important aspects.\\n\"\n",
        "                \"Demonstrates reasonable understanding but may not fully elaborate.\"\n",
        "            ),\n",
        "            2: \"General Response with Limited Details.\",\n",
        "            1: \"Minimal or Vague Response.\",\n",
        "            0: \"Unanswered\"\n",
        "        },\n",
        "        \"keywords\": {\n",
        "            4: [\"cnn\",\"convolutional\",\"pooling\",\"activation\",\"compile\",\"optimizer\",\"training\"],\n",
        "            3: [\"cnn\",\"architecture\"],\n",
        "            2: [],\n",
        "            1: []\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# ---- RUBRIC EVALUATOR ----\n",
        "def score_text_for_question(qid: int, text: str):\n",
        "    norm = normalize_text(text)\n",
        "    info = RUBRIC.get(qid)\n",
        "\n",
        "    # Kosong → nilai 0\n",
        "    if not norm or len(norm) < 5:\n",
        "        return 0, pick_one_reason(info[\"levels\"][0]), []\n",
        "\n",
        "    # LEVEL 4 → minimal 4 keyword\n",
        "    kw4 = info[\"keywords\"].get(4, [])\n",
        "    hits4 = [kw for kw in kw4 if kw in norm]\n",
        "    if len(hits4) >=4:\n",
        "        return 4, pick_one_reason(info[\"levels\"][4]), hits4\n",
        "\n",
        "    # LEVEL 3 → minimal 1 keyword\n",
        "    kw3 = info[\"keywords\"].get(3, [])\n",
        "    hits3 = [kw for kw in kw3 if kw in norm]\n",
        "    if hits3:\n",
        "        return 3, pick_one_reason(info[\"levels\"][3]), hits3\n",
        "\n",
        "    # LEVEL 2 → minimal 1 keyword\n",
        "    kw2 = info[\"keywords\"].get(2, [])\n",
        "    hits2 = [kw for kw in kw2 if kw in norm]\n",
        "    if hits2:\n",
        "        return 2, pick_one_reason(info[\"levels\"][2]), hits2\n",
        "\n",
        "    # FALLBACK\n",
        "    wc = len(norm.split())\n",
        "    if wc >= 20:\n",
        "        return 2, pick_one_reason(info[\"levels\"][2]), []\n",
        "    if wc >= 5:\n",
        "        return 1, pick_one_reason(info[\"levels\"][1]), []\n",
        "\n",
        "    return 0, pick_one_reason(info[\"levels\"][0]), []\n",
        "\n",
        "# ---- MAIN INTERVIEW LOOP ----\n",
        "results = []\n",
        "interview_total = 0\n",
        "\n",
        "for idx, wav in enumerate(wav_paths, start=1):\n",
        "    print(f\"\\n--- Processing file {idx}: {wav} ---\")\n",
        "\n",
        "    res = model.transcribe(wav, language=\"en\", verbose=False, condition_on_previous_text=False)\n",
        "\n",
        "    # remove duplicate segments\n",
        "    cleaned_segments = []\n",
        "    prev = \"\"\n",
        "    for seg in res[\"segments\"]:\n",
        "        segt = seg[\"text\"].strip()\n",
        "        if segt != prev:\n",
        "            cleaned_segments.append(segt)\n",
        "        prev = segt\n",
        "\n",
        "    transcript = \" \".join(\" \".join(cleaned_segments).split())\n",
        "\n",
        "    # confidence\n",
        "    conf = overall_confidence_from_segments(res[\"segments\"])\n",
        "\n",
        "    # WER\n",
        "    base = Path(wav).stem\n",
        "    ref = references.get(base, \"\")\n",
        "    cur_wer = wer(ref, transcript) if ref else 0.0\n",
        "    cur_acc = round((1 - cur_wer) * 100, 2)\n",
        "\n",
        "    # RUBRIC scoring\n",
        "    score, reason, evidence = score_text_for_question(idx, transcript)\n",
        "    interview_total += score\n",
        "\n",
        "    results.append({\n",
        "        \"id\": idx,\n",
        "        \"question\": QUESTIONS[idx],\n",
        "        \"transcription\": transcript,\n",
        "        \"confidence\": round(conf, 4),\n",
        "        \"wer\": cur_wer,\n",
        "        \"accuracy\": cur_acc,\n",
        "        \"score\": score,\n",
        "        \"reason\": reason,\n",
        "        \"evidence\": evidence\n",
        "    })\n",
        "\n",
        "    print(\"Transcript:\", transcript)\n",
        "    print(\"Score:\", score)\n",
        "    print(\"Reason:\", reason)\n",
        "    print(\"Evidence:\", evidence)\n",
        "    print(\"Confidence:\", conf)\n",
        "    print(\"WER:\", cur_wer)\n",
        "    print(\"Accuracy:\", cur_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a926246-4ef7-4a81-e1f5-c02c04fdc8cf",
        "id": "Uby65BBaIbc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Processing file 1: capstone_output/audios/interview_question_1.wav ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9300/9300 [00:09<00:00, 1028.80frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcript: share any specific challenges you faced while working on certification and how you overcome them ah, ok actually for the challenges, there are some challenges when I took the certification especially for the project submission that I already working with it the first one is actually to meet the specific accuracy or violation laws for the evaluation matrix and yeah actually that's just need to take some trial and error with the different architecture for example like we can try to add more layer, more neurons, changes the neurons or even I also apply the dropout layer so it really helps with the validation loss to become more lower and I think that's one of the biggest challenges that I have while working on this certification\n",
            "Score: 4\n",
            "Reason: Response is very clear, well-organized, and reflects the learning process.\n",
            "Evidence: ['challenge', 'overcome', 'error', 'architecture', 'certification']\n",
            "Confidence: 0.6701160596140153\n",
            "WER: 0.007936507936507936\n",
            "Accuracy: 99.21\n",
            "\n",
            "--- Processing file 2: capstone_output/audios/interview_question_2.wav ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11472/11472 [00:10<00:00, 1066.05frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcript: Can you describe your experience with transfer learning and time short flow? How do you benefit from the projects? About transfer learning is actually we use existing train model from time short flow for example like VGC16, VGC19 especially for some cases that we need to use deep learning using Keras applications for example like image classification we can use transfer learning models which is that's already train model with exceptionally high accuracy, high performance, even though it's trained with different datasets but it really helps to improve our model performance, model accuracy, model loss and you know for example like mobile net, VGG19, VGG16, efficient net, it will help to improve our our models comparing to the one if you use a traditional CNN model yeah CNN model with the convolutional 2d yeah max pooling and yeah it it's it's quite good actually to use transfer learning it really helps with our model performance to improve our model performance\n",
            "Score: 3\n",
            "Reason: Explains how transfer learning benefited those projects.\n",
            "Evidence: ['transfer learning']\n",
            "Confidence: 0.7292506671763449\n",
            "WER: 0.00625\n",
            "Accuracy: 99.38\n",
            "\n",
            "--- Processing file 3: capstone_output/audios/interview_question_3.wav ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11970/11970 [00:10<00:00, 1113.64frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcript: wait what is this? describe a complex TensorFlow model you have built and the steps you took to ensure its accuracy and efficiency hmmm, a complex TensorFlow model you have built and steps you took to ensure its accuracy okay I will take one of my previous project that I use yeah I also use Keras or TensorFlow model it's it is about a celiac disease prediction yeah yeah this is also I use the research project for my undergraduate thesis yeah for my script C and I use this model it's quite challenging even though it's achieved high accuracy with some dense layer, with some drawout layer and trial and error also with the callback function with the neurons but the problem is the dataset is not balanced so it has the imbalanced class datasets and the approach that I use is to just to use the technique called smooth and synthetic oversampling technique with edited nearest neighbor which is basically it's just oversampling and undersampling the datasets it helps with the accuracy\n",
            "Score: 3\n",
            "Reason: Describes a complex TensorFlow model they have built.\n",
            "Evidence: ['model', 'accuracy']\n",
            "Confidence: 0.7125486450436235\n",
            "WER: 0.005747126436781609\n",
            "Accuracy: 99.43\n",
            "\n",
            "--- Processing file 4: capstone_output/audios/interview_question_4.wav ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12000/12000 [00:11<00:00, 1008.13frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcript: explain how you implement dropout in test on flow model and test on training previously i also have implement the dropout layer also in the project submission within this certifications and we can just add the dropout layer for example if I'm not mistaken it's I have used this dropout layer in the the one that the case is image classifications yeah a German traffic something if I'm not wrong I have used this dropout layer in in in the not in the last in the middle of the layer so there's a flattened layer right not flatten the convolutional layer and then the flattened layer and I use that dropout layer which is I use with the rate of 0.2 or 0.5 if I'm not wrong and then the dense layer and the last the output layer right the effect is it will really helps to improve our accuracy and lower our validation loss by turning off some of the previous layer, for example like we have dense layer 64 and the next layer we implement the dropout layer with the rate of 0.5 and it will turn off randomly each epoch of the previous dense layer\n",
            "Score: 4\n",
            "Reason: Discusses the impact on model performance, generalization, and possibly mentions considerations like dropout rates.\n",
            "Evidence: ['dropout', 'rate', 'random', 'turning', 'off']\n",
            "Confidence: 0.770766696852009\n",
            "WER: 0.0\n",
            "Accuracy: 100.0\n",
            "\n",
            "--- Processing file 5: capstone_output/audios/interview_question_5.wav ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12000/12000 [00:12<00:00, 950.02frames/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcript: Describe the process of building or configuration of your image application okay, the CNN one, right? so, at the first time, of course, we need to make sure there are split, the image folder is split for each class and then we can use Keras preprocessing image dataset from directory to split the training and the validation dataset of course we can use another set which is the test dataset but yeah okay the next one we can we can just maybe we we need to implement also the image augmentation yeah data data image data augmentation to to make our data set more veritive right for example like we can rotate we can zoom it we can crop it yeah and yeah the last thing yeah of course we we can build our chain model with the convolutional 2D specify the filters the kernel size the LR activation of course the input shape for the first layer and then we can apply the max pooling 2D yeah and the next layer we can just use use conflux 2d, mix pooling and whatever it is and after that we apply the flatten layer and dropout layer if you want and the last thing don't forget to use the dance layer right for the output\n",
            "Score: 4\n",
            "Reason: Comprehensive and Very Clear Response.\n",
            "Evidence: ['cnn', 'convolutional', 'pooling', 'activation', 'training']\n",
            "Confidence: 0.70986632599124\n",
            "WER: 0.004672897196261682\n",
            "Accuracy: 99.53\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_payload = {\n",
        "    \"assessorProfile\": {\n",
        "        \"id\": 47,\n",
        "        \"name\": \"AutoSystem\",\n",
        "        \"photoURL\": \"auto.png\"\n",
        "    },\n",
        "    \"decision\": \"Need Human\" if interview_total < 16 else \"PASSED\",\n",
        "    \"scoresOverview\": {\n",
        "        \"project\": 100,\n",
        "        \"interview\": interview_total,\n",
        "        \"total\": 94.3\n",
        "    },\n",
        "    \"reviewChecklistResult\": {\n",
        "        \"interview\": {\n",
        "            \"minScore\": 0,\n",
        "            \"maxScore\": 4,\n",
        "            \"scores\": results\n",
        "        }\n",
        "    },\n",
        "    \"overallNotes\": \"Interview responses appear consistent.\"\n",
        "}\n",
        "\n",
        "out_path = os.path.join(OUT_DIR, \"FINAL_PAYLOAD.json\")\n",
        "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(final_payload, f, indent=2)\n",
        "\n",
        "print(\"Saved:\", out_path)\n",
        "print(\"Interview total score:\", interview_total)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3e841ac-d7e0-4571-c63b-19063b981170",
        "id": "-SG2KOxzIbc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: capstone_output/FINAL_PAYLOAD.json\n",
            "Interview total score: 18\n"
          ]
        }
      ]
    }
  ]
}